{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ExCaLBBR/ExCaLBBR_Projects/blob/main/PartisanBiasDetection/LocalScraperTemplate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This is the automated scraper for FoxNews(Digital)**\n",
        "\n",
        "When addapting this script for other news sources you will need to:\n",
        "*   Change the uril point you to the rss feed or breaking news webpage\n",
        "*   Change the suffix in the parameter section"
      ],
      "metadata": {
        "id": "Y4Bc2vB3c6_8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "jzf4wY-T8Frl"
      },
      "outputs": [],
      "source": [
        "#@title Install Dependancies\n",
        "!pip install requests beautifulsoup4 --quiet\n",
        "\n",
        "#Import libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import datetime,timezone\n",
        "import pytz\n",
        "from pathlib import Path\n",
        "\n",
        "est_tz = pytz.timezone('US/Eastern')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define Utility Functions\n",
        "\n",
        "#Article Scrapper\n",
        "def scrape_article_text(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Find elements containing text - this part may need customization\n",
        "        # For example, assuming article text is within <p> tags\n",
        "        article_text = ' '.join(p.get_text() for p in soup.find_all('p'))\n",
        "        return article_text\n",
        "    else:\n",
        "        return \"Error: Unable to fetch the webpage.\"\n",
        "\n",
        "#Article Scraper with User Agent - add to utility functions\n",
        "def scrape_article_text_useragent(url,ua):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url, headers=ua)\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Find elements containing text - this part may need customization\n",
        "        # For example, assuming article text is within <p> tags\n",
        "        article_text = ' '.join(p.get_text() for p in soup.find_all('p'))\n",
        "        return article_text\n",
        "    else:\n",
        "        return \"Error: Unable to fetch the webpage.\"\n"
      ],
      "metadata": {
        "id": "-1vSCujtWffV",
        "cellView": "form"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parameter specification\n",
        "sourceSuffix = 'FoxNews'"
      ],
      "metadata": {
        "id": "AS6kcuVstVp-"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fox News Article Text Link Scraper\n",
        "url = 'https://moxie.foxnews.com/google-publisher/politics.xml'\n",
        "html_page = requests.get(url)\n",
        "soup = BeautifulSoup(html_page.text, \"lxml\")\n",
        "links = []\n",
        "artTxt = []\n",
        "for item in soup.find_all(\"item\"):\n",
        "  link= str(item)\n",
        "  i = link.find(\"<link/>\")\n",
        "  j = link.find(\"<guid\")\n",
        "  x = link[i+7:j]\n",
        "  y = x.split('\\n', 1)[0]\n",
        "  links.append(y)\n",
        "  article_text = scrape_article_text(y)\n",
        "  artTxt.append(article_text)\n",
        "\n",
        "#Fox News Article Header Scraper\n",
        "headers = []\n",
        "for item in soup.find_all(\"item\"):\n",
        "  header= str(item)\n",
        "  i = header.find(\"<title>\")\n",
        "  j = header.find(\"</title>\")\n",
        "  x = header[i+7:j]\n",
        "  y = x.split('\\n', 1)[0]\n",
        "  headers.append(y)\n",
        "\n",
        "#Fox News Article PubDate Scraper\n",
        "pubds = []\n",
        "for item in soup.find_all(\"item\"):\n",
        "  pubd= str(item)\n",
        "  i = pubd.find(\"<pubdate>\")\n",
        "  j = pubd.find(\"</pubdate>\")\n",
        "  x = pubd[i+9:j]\n",
        "  y = x.split('\\n', 1)[0]\n",
        "  pubds.append(y)\n",
        "\n",
        "d = {'Links': links, 'ArticleHeaders': headers, 'ArticleText': artTxt, 'PubDate': pubds}\n",
        "artD = pd.DataFrame(d)\n"
      ],
      "metadata": {
        "id": "5mml0I9kwLNy",
        "outputId": "0ba0ae54-f2d6-4ccf-918b-135350520b76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-67-3c27d6710d64>:4: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "  soup = BeautifulSoup(html_page.text, \"lxml\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define event times (UTC), then convert to EST\n",
        "event_time_utc_0 = datetime(9999, 12, 31, 5, 0, 0, tzinfo=pytz.utc)  # Event before in UTC\n",
        "event_time_utc_6 = datetime(9999, 12, 31, 11, 0, 0, tzinfo=pytz.utc)     # Event after in UTC\n",
        "event_time_utc_12 = datetime(9999, 12, 31, 16, 0, 0, tzinfo=pytz.utc)  # Event before in UTC\n",
        "event_time_utc_16 = datetime(9999, 12, 31, 21, 0, 0, tzinfo=pytz.utc)     # Event after in UTC\n",
        "event_time_utc_EoD = datetime(9999, 12, 31, 4, 59, 0, tzinfo=pytz.utc)     # Event after in UTC\n",
        "\n",
        "# Convert to EST and extract time\n",
        "event_time_est_0 = event_time_utc_0.astimezone(est_tz).time()\n",
        "event_time_est_6 = event_time_utc_6.astimezone(est_tz).time()\n",
        "event_time_est_12 = event_time_utc_12.astimezone(est_tz).time()\n",
        "event_time_est_16 = event_time_utc_16.astimezone(est_tz).time()\n",
        "event_time_est_EoD = event_time_utc_EoD.astimezone(est_tz).time()"
      ],
      "metadata": {
        "id": "nCsJnaFQuJ4m"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Determine outcome based on time of day\n",
        "utc_now = datetime.now(timezone.utc)  # Global time in UTC\n",
        "est_now = utc_now.astimezone(est_tz).time()  # Convert to EST and extract\n",
        "\n",
        "if est_now > event_time_est_0 and est_now < event_time_est_6:\n",
        "  artD.to_csv('DayFiles/DayFile_' + sourceSuffix + '.tsv', sep=\"\\t\") #Define DayFile\n",
        "\n",
        "elif est_now > event_time_est_6 and est_now < event_time_est_12:\n",
        "  i = 1+2\n",
        "  #Load DayFile, find unq, and append with new dataframe\n",
        "  #Save over previous DayFile\n",
        "\n",
        "elif est_now > event_time_est_12 and est_now < event_time_est_16:\n",
        "  j = 1+2\n",
        "  #Load DayFile, find unq, and append with new dataframe\n",
        "  #Save over previous DayFile\n",
        "\n",
        "elif est_now > event_time_est_16 and est_now < event_time_est_EoD:\n",
        "  w = 1+2\n",
        "  #Load Master file\n",
        "  #Load DayFile, find unq, and append with new dataframe\n",
        "  #Find unq Day and Master\n",
        "  #Append Master\n",
        "  #Save new makster"
      ],
      "metadata": {
        "id": "rU0gOoQOfRTk"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # my_file = Path('DayFiles/DayFile_' + sourceSuffix + '.tsv')\n",
        "  # if my_file.is_file():"
      ],
      "metadata": {
        "id": "zDkZuD4aDXXa",
        "outputId": "85781d4e-491c-4749-cad8-1fc5711947f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    }
  ]
}