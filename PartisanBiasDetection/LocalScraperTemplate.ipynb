{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ExCaLBBR/ExCaLBBR_Projects/blob/main/PartisanBiasDetection/LocalScraperTemplate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This is the automated scraper for FoxNews(Digital)**\n",
        "\n",
        "When addapting this script for other news sources you will need to:\n",
        "*   Change the uril point you to the rss feed or breaking news webpage\n",
        "*   Change the suffix in the parameter section"
      ],
      "metadata": {
        "id": "Y4Bc2vB3c6_8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "jzf4wY-T8Frl"
      },
      "outputs": [],
      "source": [
        "#@title Install Dependancies\n",
        "!pip install requests beautifulsoup4 --quiet\n",
        "\n",
        "#Import libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "from pathlib import Path\n",
        "\n",
        "est_tz = pytz.timezone('US/Eastern')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define Utility Functions\n",
        "\n",
        "#Article Scrapper\n",
        "def scrape_article_text(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Find elements containing text - this part may need customization\n",
        "        # For example, assuming article text is within <p> tags\n",
        "        article_text = ' '.join(p.get_text() for p in soup.find_all('p'))\n",
        "        return article_text\n",
        "    else:\n",
        "        return \"Error: Unable to fetch the webpage.\"\n",
        "\n",
        "#Article Scraper with User Agent - add to utility functions\n",
        "def scrape_article_text_useragent(url,ua):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url, headers=ua)\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Find elements containing text - this part may need customization\n",
        "        # For example, assuming article text is within <p> tags\n",
        "        article_text = ' '.join(p.get_text() for p in soup.find_all('p'))\n",
        "        return article_text\n",
        "    else:\n",
        "        return \"Error: Unable to fetch the webpage.\"\n"
      ],
      "metadata": {
        "id": "-1vSCujtWffV",
        "cellView": "form"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parameter specification\n",
        "sourceSuffix = 'FoxNews'"
      ],
      "metadata": {
        "id": "AS6kcuVstVp-"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fox News Article Text Link Scraper\n",
        "url = 'https://moxie.foxnews.com/google-publisher/politics.xml'\n",
        "html_page = requests.get(url)\n",
        "soup = BeautifulSoup(html_page.text, \"lxml\")\n",
        "links = []\n",
        "artTxt = []\n",
        "for item in soup.find_all(\"item\"):\n",
        "  link= str(item)\n",
        "  i = link.find(\"<link/>\")\n",
        "  j = link.find(\"<guid\")\n",
        "  x = link[i+7:j]\n",
        "  y = x.split('\\n', 1)[0]\n",
        "  links.append(y)\n",
        "  article_text = scrape_article_text(y)\n",
        "  artTxt.append(article_text)\n",
        "\n",
        "#Fox News Article Header Scraper\n",
        "headers = []\n",
        "for item in soup.find_all(\"item\"):\n",
        "  header= str(item)\n",
        "  i = header.find(\"<title>\")\n",
        "  j = header.find(\"</title>\")\n",
        "  x = header[i+7:j]\n",
        "  y = x.split('\\n', 1)[0]\n",
        "  headers.append(y)\n",
        "\n",
        "#Fox News Article PubDate Scraper\n",
        "pubds = []\n",
        "for item in soup.find_all(\"item\"):\n",
        "  pubd= str(item)\n",
        "  i = pubd.find(\"<pubdate>\")\n",
        "  j = pubd.find(\"</pubdate>\")\n",
        "  x = pubd[i+9:j]\n",
        "  y = x.split('\\n', 1)[0]\n",
        "  pubds.append(y)\n",
        "\n",
        "d = {'Links': links, 'ArticleHeaders': headers, 'ArticleText': artTxt, 'PubDate': pubds}\n",
        "artD = pd.DataFrame(d)\n"
      ],
      "metadata": {
        "id": "5mml0I9kwLNy",
        "outputId": "0ba0ae54-f2d6-4ccf-918b-135350520b76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-67-3c27d6710d64>:4: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "  soup = BeautifulSoup(html_page.text, \"lxml\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Event times (UTC), then convert to EST\n",
        "event_time_utc_before = datetime(9999, 12, 31, 4, 0, 0, tzinfo=pytz.utc)  # Event before in UTC\n",
        "event_time_utc_after = datetime(9999, 12, 31, 10, 0, 0, tzinfo=pytz.utc)     # Event after in UTC\n",
        "event_time_est_before = event_time_utc_before.astimezone(est_tz).time()  # Convert to EST and extract time\n",
        "event_time_est_after = event_time_utc_after.astimezone(est_tz).time()"
      ],
      "metadata": {
        "id": "nCsJnaFQuJ4m"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "utc_now = datetime.now()  # Global time in UTC\n",
        "est_now = utc_now.astimezone(est_tz).time()  # Convert to EST and extract"
      ],
      "metadata": {
        "id": "h-xFtIXjy8IP"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "utc_now"
      ],
      "metadata": {
        "id": "SmYhwNeTzYsv",
        "outputId": "55bc3f53-121e-443d-fa4d-c4da60257610",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.datetime(2024, 9, 23, 16, 53, 22, 400179)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "est_now"
      ],
      "metadata": {
        "id": "ZEMveDz1zVnn",
        "outputId": "dcf40779-06dd-45ed-8dd6-3b054033bd44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.time(12, 53, 22, 400179)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Determine outcome based on time of day\n",
        "utc_now = datetime.now()  # Global time in UTC\n",
        "est_now = utc_now.astimezone(est_tz).time()  # Convert to EST and extract\n",
        "\n",
        "# Define interval thresholds\n",
        "event_time_utc_before = datetime(9999, 12, 31, 4, 0, 0, tzinfo=pytz.utc)  # Event before in UTC\n",
        "event_time_utc_after = datetime(9999, 12, 31, 10, 0, 0, tzinfo=pytz.utc)     # Event after in UTC\n",
        "event_time_est_before = event_time_utc_before.astimezone(est_tz).time()  # Convert to EST and extract time\n",
        "event_time_est_after = event_time_utc_after.astimezone(est_tz).time()\n",
        "\n",
        "if est_now > event_time_est_before and event_time_est_after < 6: #offset hour by 4 to account UTC to EST\n",
        "\n",
        "  my_file = Path('DayFiles/DayFile_' + sourceSuffix + '.tsv')\n",
        "  if my_file.is_file():\n",
        "    artD.to_csv('DayFile_' + sourceSuffix + '.tsv', sep=\"\\t\")\n",
        "\n",
        "#elif est_now_hr > 6 and est_now_hr < 12: #offset hour by 4 to account UTC to EST\n",
        "\n",
        "elif est_now > 12 and est_now < 16: #offset hour by 4 to account UTC to EST\n",
        "  artD.to_csv('DayFile_' + sourceSuffix + '.tsv', sep=\"\\t\")"
      ],
      "metadata": {
        "id": "rU0gOoQOfRTk"
      },
      "execution_count": 75,
      "outputs": []
    }
  ]
}